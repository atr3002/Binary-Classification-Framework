{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TASK 2: BINARY CLASSIFICATION MODELS"
      ],
      "metadata": {
        "id": "HNMkAJj-wSRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                           roc_auc_score, confusion_matrix, classification_report,\n",
        "                           roc_curve, precision_recall_curve, auc)\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import joblib\n",
        "import time\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "_0LMJbSOwUdn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"STEP 1: DATA LOADING AND PREPROCESSING\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UJpYLTvmwbhI",
        "outputId": "7f14a79d-5a59-4af0-984c-bc4bbcfd143e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STEP 1: DATA LOADING AND PREPROCESSING\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"blastchar/telco-customer-churn\")\n",
        "\n",
        "# Exact filename\n",
        "print(os.listdir(path))\n",
        "\n",
        "# Load the CSV\n",
        "df = pd.read_csv(os.path.join(path, \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq_4pKqpxL5z",
        "outputId": "b734d5d4-3f55-496a-dbc0-a8efa92230c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'telco-customer-churn' dataset.\n",
            "['WA_Fn-UseC_-Telco-Customer-Churn.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\\n\")\n",
        "\n",
        "# Data cleaning based on Task 1 findings\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "\n",
        "# Handle missing values (using non-churned median as identified in Task 1)\n",
        "non_churned_median = df[df['Churn'] == 'No']['TotalCharges'].median()\n",
        "df['TotalCharges'] = df['TotalCharges'].fillna(non_churned_median)\n",
        "\n",
        "print(f\"Missing values after imputation: {df['TotalCharges'].isnull().sum()}\")\n",
        "\n",
        "# Drop customerID as it's not useful for prediction\n",
        "df = df.drop('customerID', axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pipQfevIxQ5Q",
        "outputId": "f9beba5a-acf9-4177-935e-9bf97e4934dc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (7043, 21)\n",
            "Columns: ['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']\n",
            "\n",
            "Missing values after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"STEP 2: FEATURE ENGINEERING\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw9r7RzTxfE1",
        "outputId": "c5eca384-f0a3-40d3-98c5-4d1a1ad5387e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================================================================\n",
            "STEP 2: FEATURE ENGINEERING\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new features based on EDA insights\n",
        "df['TenureGroup'] = pd.cut(df['tenure'],\n",
        "                           bins=[0, 12, 24, 36, 48, 60, 72],\n",
        "                           labels=['0-12m', '13-24m', '25-36m', '37-48m', '49-60m', '61-72m'])\n",
        "\n",
        "# Calculate charge ratio\n",
        "df['ChargeRatio'] = df['MonthlyCharges'] / (df['TotalCharges'] + 1)  # +1 to avoid division by zero\n",
        "\n",
        "# Create total services feature\n",
        "service_columns = ['PhoneService', 'MultipleLines', 'InternetService',\n",
        "                   'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
        "                   'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
        "\n",
        "df['TotalServices'] = 0\n",
        "for col in service_columns:\n",
        "    if col in df.columns:\n",
        "        # Convert categorical to binary\n",
        "        df['TotalServices'] += (df[col] != 'No').astype(int)\n",
        "\n",
        "print(\"New features created:\")\n",
        "print(f\"- TenureGroup: {df['TenureGroup'].unique()}\")\n",
        "print(f\"- ChargeRatio: Range [{df['ChargeRatio'].min():.3f}, {df['ChargeRatio'].max():.3f}]\")\n",
        "print(f\"- TotalServices: Range [{df['TotalServices'].min()}, {df['TotalServices'].max()}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dbK-76VxgH0",
        "outputId": "d4331a14-4d6d-4a0a-d5f0-3688de305281"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New features created:\n",
            "- TenureGroup: ['0-12m', '25-36m', '37-48m', '13-24m', '61-72m', '49-60m', NaN]\n",
            "Categories (6, object): ['0-12m' < '13-24m' < '25-36m' < '37-48m' < '49-60m' < '61-72m']\n",
            "- ChargeRatio: Range [0.012, 0.990]\n",
            "- TotalServices: Range [2, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"STEP 3: DATA PREPARATION FOR MODELING\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCLROTaIx4vi",
        "outputId": "5deab061-554f-49a2-ff6f-90763801a646"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================================================================\n",
            "STEP 3: DATA PREPARATION FOR MODELING\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target\n",
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']\n",
        "\n",
        "# Encode target variable\n",
        "y = y.map({'No': 0, 'Yes': 1})\n",
        "\n",
        "# Identify feature types\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "print(f\"Categorical features: {categorical_cols}\")\n",
        "print(f\"Numerical features: {numerical_cols}\")\n",
        "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
        "print(f\"Churn rate: {(y.sum() / len(y)) * 100:.2f}%\")\n",
        "\n",
        "# Split data with stratification to maintain class balance\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")\n",
        "print(f\"Train churn rate: {(y_train.sum() / len(y_train)) * 100:.2f}%\")\n",
        "print(f\"Test churn rate: {(y_test.sum() / len(y_test)) * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSja71fCx7_I",
        "outputId": "9bee27f4-8858-4709-9a43-1d1e0d66cc3d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical features: ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'TenureGroup']\n",
            "Numerical features: ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges', 'ChargeRatio', 'TotalServices']\n",
            "Target distribution:\n",
            "Churn\n",
            "0    5174\n",
            "1    1869\n",
            "Name: count, dtype: int64\n",
            "Churn rate: 26.54%\n",
            "\n",
            "Train set size: (5634, 22)\n",
            "Test set size: (1409, 22)\n",
            "Train churn rate: 26.54%\n",
            "Test churn rate: 26.54%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"STEP 4: PREPROCESSING PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Define preprocessing for numerical features\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # Handle any remaining missing values\n",
        "    ('scaler', StandardScaler())  # Standardize features\n",
        "])\n",
        "\n",
        "# Define preprocessing for categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing values\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encoding\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Preprocessing pipeline created successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scsi5lj0yV56",
        "outputId": "8e4b47dd-6bff-4269-90c4-280195f0049a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================================================================\n",
            "STEP 4: PREPROCESSING PIPELINE\n",
            "======================================================================\n",
            "Preprocessing pipeline created successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"STEP 5: DECISION TREE CLASSIFIER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create Decision Tree pipeline\n",
        "dt_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Define hyperparameter grid for Decision Tree\n",
        "dt_param_grid = {\n",
        "    'classifier__criterion': ['gini', 'entropy'],\n",
        "    'classifier__max_depth': [3, 5, 7, 10, 15, None],\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4],\n",
        "    'classifier__max_features': ['sqrt', 'log2', None],\n",
        "    'classifier__class_weight': ['balanced', None]\n",
        "}\n",
        "\n",
        "print(\"Performing Grid Search for Decision Tree...\")\n",
        "\n",
        "# Perform GridSearchCV with cross-validation\n",
        "dt_grid_search = GridSearchCV(\n",
        "    dt_pipeline,\n",
        "    dt_param_grid,\n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "dt_grid_search.fit(X_train, y_train)\n",
        "dt_train_time = time.time() - start_time\n",
        "\n",
        "print(f\"Grid Search completed in {dt_train_time:.2f} seconds\")\n",
        "\n",
        "# Get best model and parameters\n",
        "dt_best_model = dt_grid_search.best_estimator_\n",
        "dt_best_params = dt_grid_search.best_params_\n",
        "\n",
        "print(\"\\nBest Decision Tree Parameters:\")\n",
        "for param, value in dt_best_params.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_dt = dt_best_model.predict(X_test)\n",
        "y_pred_proba_dt = dt_best_model.predict_proba(X_test)[:, 1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHQWV1s6ydqG",
        "outputId": "a8358646-8b64-4150-f637-e32f96da9dc5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================================================================\n",
            "STEP 5: DECISION TREE CLASSIFIER\n",
            "======================================================================\n",
            "Performing Grid Search for Decision Tree...\n",
            "Fitting 5 folds for each of 648 candidates, totalling 3240 fits\n",
            "Grid Search completed in 288.12 seconds\n",
            "\n",
            "Best Decision Tree Parameters:\n",
            "  classifier__class_weight: balanced\n",
            "  classifier__criterion: gini\n",
            "  classifier__max_depth: 3\n",
            "  classifier__max_features: None\n",
            "  classifier__min_samples_leaf: 1\n",
            "  classifier__min_samples_split: 2\n"
          ]
        }
      ]
    }
  ]
}