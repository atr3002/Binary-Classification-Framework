{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TASK 2: BINARY CLASSIFICATION MODELS"
      ],
      "metadata": {
        "id": "HNMkAJj-wSRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                           roc_auc_score, confusion_matrix, classification_report,\n",
        "                           roc_curve, precision_recall_curve, auc)\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import joblib\n",
        "import time\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "_0LMJbSOwUdn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"STEP 1: DATA LOADING AND PREPROCESSING\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UJpYLTvmwbhI",
        "outputId": "cff14b68-5dc6-49b8-e484-d386e7e8f43d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STEP 1: DATA LOADING AND PREPROCESSING\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"blastchar/telco-customer-churn\")\n",
        "\n",
        "# Exact filename\n",
        "print(os.listdir(path))\n",
        "\n",
        "# Load the CSV\n",
        "df = pd.read_csv(os.path.join(path, \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq_4pKqpxL5z",
        "outputId": "5ea9995d-f324-4fdd-da6c-b79e546004b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'telco-customer-churn' dataset.\n",
            "['WA_Fn-UseC_-Telco-Customer-Churn.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\\n\")\n",
        "\n",
        "# Data cleaning based on Task 1 findings\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "\n",
        "# Handle missing values (using non-churned median as identified in Task 1)\n",
        "non_churned_median = df[df['Churn'] == 'No']['TotalCharges'].median()\n",
        "df['TotalCharges'] = df['TotalCharges'].fillna(non_churned_median)\n",
        "\n",
        "print(f\"Missing values after imputation: {df['TotalCharges'].isnull().sum()}\")\n",
        "\n",
        "# Drop customerID as it's not useful for prediction\n",
        "df = df.drop('customerID', axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pipQfevIxQ5Q",
        "outputId": "b5153c71-901e-4e83-9366-94aa8e07daef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (7043, 21)\n",
            "Columns: ['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']\n",
            "\n",
            "Missing values after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"STEP 2: FEATURE ENGINEERING\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw9r7RzTxfE1",
        "outputId": "7278f12a-dfee-4c8f-ccb3-ce5fe99dc7cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================================================================\n",
            "STEP 2: FEATURE ENGINEERING\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new features based on EDA insights\n",
        "df['TenureGroup'] = pd.cut(df['tenure'],\n",
        "                           bins=[0, 12, 24, 36, 48, 60, 72],\n",
        "                           labels=['0-12m', '13-24m', '25-36m', '37-48m', '49-60m', '61-72m'])\n",
        "\n",
        "# Calculate charge ratio\n",
        "df['ChargeRatio'] = df['MonthlyCharges'] / (df['TotalCharges'] + 1)  # +1 to avoid division by zero\n",
        "\n",
        "# Create total services feature\n",
        "service_columns = ['PhoneService', 'MultipleLines', 'InternetService',\n",
        "                   'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
        "                   'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
        "\n",
        "df['TotalServices'] = 0\n",
        "for col in service_columns:\n",
        "    if col in df.columns:\n",
        "        # Convert categorical to binary\n",
        "        df['TotalServices'] += (df[col] != 'No').astype(int)\n",
        "\n",
        "print(\"New features created:\")\n",
        "print(f\"- TenureGroup: {df['TenureGroup'].unique()}\")\n",
        "print(f\"- ChargeRatio: Range [{df['ChargeRatio'].min():.3f}, {df['ChargeRatio'].max():.3f}]\")\n",
        "print(f\"- TotalServices: Range [{df['TotalServices'].min()}, {df['TotalServices'].max()}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dbK-76VxgH0",
        "outputId": "ea0e7f52-a8e3-42b4-e3e2-88fece582a02"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New features created:\n",
            "- TenureGroup: ['0-12m', '25-36m', '37-48m', '13-24m', '61-72m', '49-60m', NaN]\n",
            "Categories (6, object): ['0-12m' < '13-24m' < '25-36m' < '37-48m' < '49-60m' < '61-72m']\n",
            "- ChargeRatio: Range [0.012, 0.990]\n",
            "- TotalServices: Range [2, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"STEP 3: DATA PREPARATION FOR MODELING\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCLROTaIx4vi",
        "outputId": "428695c9-369a-4e01-ac28-12fae3112e64"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================================================================\n",
            "STEP 3: DATA PREPARATION FOR MODELING\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target\n",
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']\n",
        "\n",
        "# Encode target variable\n",
        "y = y.map({'No': 0, 'Yes': 1})\n",
        "\n",
        "# Identify feature types\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "print(f\"Categorical features: {categorical_cols}\")\n",
        "print(f\"Numerical features: {numerical_cols}\")\n",
        "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
        "print(f\"Churn rate: {(y.sum() / len(y)) * 100:.2f}%\")\n",
        "\n",
        "# Split data with stratification to maintain class balance\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")\n",
        "print(f\"Train churn rate: {(y_train.sum() / len(y_train)) * 100:.2f}%\")\n",
        "print(f\"Test churn rate: {(y_test.sum() / len(y_test)) * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VSja71fCx7_I",
        "outputId": "488eae60-ca83-4aa9-f30d-c2f8eba7980f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical features: ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'TenureGroup']\n",
            "Numerical features: ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges', 'ChargeRatio', 'TotalServices']\n",
            "Target distribution:\n",
            "Churn\n",
            "0    5174\n",
            "1    1869\n",
            "Name: count, dtype: int64\n",
            "Churn rate: 26.54%\n",
            "\n",
            "Train set size: (5634, 22)\n",
            "Test set size: (1409, 22)\n",
            "Train churn rate: 26.54%\n",
            "Test churn rate: 26.54%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"STEP 4: PREPROCESSING PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Define preprocessing for numerical features\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # Handle any remaining missing values\n",
        "    ('scaler', StandardScaler())  # Standardize features\n",
        "])\n",
        "\n",
        "# Define preprocessing for categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing values\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encoding\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Preprocessing pipeline created successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scsi5lj0yV56",
        "outputId": "352f8b7b-7e4c-4ad8-d3de-6f4f6428d9e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================================================================\n",
            "STEP 4: PREPROCESSING PIPELINE\n",
            "======================================================================\n",
            "Preprocessing pipeline created successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# STEP 5: DECISION TREE MODEL (OPTIMIZED)\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"STEP 5: DECISION TREE CLASSIFIER - OPTIMIZED\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create Decision Tree pipeline\n",
        "dt_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', DecisionTreeClassifier(random_state=42, class_weight='balanced'))\n",
        "])\n",
        "\n",
        "# OPTIMIZATION 1: Reduced hyperparameter grid\n",
        "dt_param_grid_optimized = {\n",
        "    'classifier__criterion': ['gini', 'entropy'],\n",
        "    'classifier__max_depth': [3, 5, 7, None],  # Added None for unlimited\n",
        "    'classifier__min_samples_split': [2, 10, 20],  # Reduced options\n",
        "    'classifier__min_samples_leaf': [1, 4, 10],  # Reduced options\n",
        "    'classifier__max_features': ['sqrt', None],  # Reduced options\n",
        "}\n",
        "\n",
        "print(\"Optimized Hyperparameter Grid for Decision Tree:\")\n",
        "for param, values in dt_param_grid_optimized.items():\n",
        "    print(f\"  {param}: {values}\")\n",
        "\n",
        "print(\"\\nTotal combinations to test: \", end=\"\")\n",
        "total_combos = 1\n",
        "for values in dt_param_grid_optimized.values():\n",
        "    total_combos *= len(values)\n",
        "print(f\"{total_combos} (vs 108 in original)\")\n",
        "\n",
        "print(\"\\nPerforming OPTIMIZED Grid Search for Decision Tree...\")\n",
        "print(\"This will be significantly faster...\")\n",
        "\n",
        "# OPTIMIZATION 2: Use RandomizedSearchCV instead of GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "dt_random_search = RandomizedSearchCV(\n",
        "    dt_pipeline,\n",
        "    dt_param_grid_optimized,\n",
        "    n_iter=15,  # Test only 15 random combinations (much faster!)\n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,  # Use all CPU cores\n",
        "    random_state=42,\n",
        "    verbose=1  # Show progress\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "dt_random_search.fit(X_train, y_train)\n",
        "dt_train_time = time.time() - start_time\n",
        "\n",
        "print(f\"✓ Randomized Search completed in {dt_train_time:.2f} seconds\")\n",
        "\n",
        "# Get best model and parameters\n",
        "dt_best_model = dt_random_search.best_estimator_\n",
        "dt_best_params = dt_random_search.best_params_\n",
        "\n",
        "print(\"\\nBest Decision Tree Parameters:\")\n",
        "for param, value in dt_best_params.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "print(f\"\\nBest CV Score (F1): {dt_random_search.best_score_:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_dt = dt_best_model.predict(X_test)\n",
        "y_pred_proba_dt = dt_best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"✓ Decision Tree predictions ready for evaluation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok_q1fdKpNtI",
        "outputId": "8a526937-144e-4c27-880e-7d9d0f557a77"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================================================================\n",
            "STEP 5: DECISION TREE CLASSIFIER - OPTIMIZED\n",
            "======================================================================\n",
            "Optimized Hyperparameter Grid for Decision Tree:\n",
            "  classifier__criterion: ['gini', 'entropy']\n",
            "  classifier__max_depth: [3, 5, 7, None]\n",
            "  classifier__min_samples_split: [2, 10, 20]\n",
            "  classifier__min_samples_leaf: [1, 4, 10]\n",
            "  classifier__max_features: ['sqrt', None]\n",
            "\n",
            "Total combinations to test: 144 (vs 108 in original)\n",
            "\n",
            "Performing OPTIMIZED Grid Search for Decision Tree...\n",
            "This will be significantly faster...\n",
            "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
            "✓ Randomized Search completed in 10.26 seconds\n",
            "\n",
            "Best Decision Tree Parameters:\n",
            "  classifier__min_samples_split: 2\n",
            "  classifier__min_samples_leaf: 4\n",
            "  classifier__max_features: None\n",
            "  classifier__max_depth: 3\n",
            "  classifier__criterion: gini\n",
            "\n",
            "Best CV Score (F1): 0.6189\n",
            "✓ Decision Tree predictions ready for evaluation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# STEP 6: NEURAL NETWORK MODEL (OPTIMIZED VERSION)\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"STEP 6: NEURAL NETWORK CLASSIFIER - OPTIMIZED\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Calculate class weights for imbalanced data\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "print(f\"Class weights for Neural Network: {class_weight_dict}\")\n",
        "\n",
        "# OPTIMIZATION 1: Reduce dataset size for hyperparameter tuning\n",
        "# Use a sample for faster grid search\n",
        "if len(X_train) > 2000:\n",
        "    X_train_sample = X_train.sample(2000, random_state=42)\n",
        "    y_train_sample = y_train.loc[X_train_sample.index]\n",
        "    print(f\"Using sampled data for GridSearch: {len(X_train_sample)} samples (original: {len(X_train)})\")\n",
        "else:\n",
        "    X_train_sample = X_train\n",
        "    y_train_sample = y_train\n",
        "\n",
        "# OPTIMIZATION 2: Simplified hyperparameter grid\n",
        "nn_param_grid_optimized = {\n",
        "    'classifier__hidden_layer_sizes': [(50,), (100,)],  # Reduced options\n",
        "    'classifier__activation': ['relu'],  # Most common, best performing\n",
        "    'classifier__alpha': [0.001, 0.01],  # Reduced options\n",
        "    'classifier__solver': ['adam'],  # Best default for most cases\n",
        "    'classifier__batch_size': [64, 128]  # Reduced options\n",
        "}\n",
        "\n",
        "print(\"\\nOptimized Hyperparameter Grid:\")\n",
        "for param, values in nn_param_grid_optimized.items():\n",
        "    print(f\"  {param}: {values}\")\n",
        "\n",
        "# Create Neural Network pipeline\n",
        "nn_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', MLPClassifier(\n",
        "        random_state=42,\n",
        "        max_iter=300,  # Reduced iterations (early stopping will handle)\n",
        "        early_stopping=True,  # Early stopping to prevent overfitting\n",
        "        validation_fraction=0.1,  # Use 10% for validation\n",
        "        n_iter_no_change=10,  # Stop if no improvement for 10 epochs\n",
        "        verbose=False\n",
        "    ))\n",
        "])\n",
        "\n",
        "print(\"\\nPerforming OPTIMIZED Grid Search for Neural Network...\")\n",
        "print(\"This will be much faster than the original version...\")\n",
        "\n",
        "# OPTIMIZATION 3: Reduced cross-validation folds\n",
        "nn_grid_search = GridSearchCV(\n",
        "    nn_pipeline,\n",
        "    nn_param_grid_optimized,\n",
        "    cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),  # Reduced from 5 to 3\n",
        "    scoring='f1',\n",
        "    n_jobs=1,  # Use 1 job to avoid memory issues (you can increase if you have more RAM)\n",
        "    verbose=2,  # Show progress\n",
        "    error_score='raise'\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "nn_grid_search.fit(X_train_sample, y_train_sample)\n",
        "nn_train_time = time.time() - start_time\n",
        "\n",
        "print(f\"✓ Grid Search completed in {nn_train_time/60:.2f} minutes ({nn_train_time:.2f} seconds)\")\n",
        "\n",
        "# Get best model and parameters\n",
        "nn_best_model = nn_grid_search.best_estimator_\n",
        "nn_best_params = nn_grid_search.best_params_\n",
        "\n",
        "print(\"\\nBest Neural Network Parameters:\")\n",
        "for param, value in nn_best_params.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "print(f\"\\nBest CV Score (F1): {nn_grid_search.best_score_:.4f}\")\n",
        "\n",
        "# OPTIMIZATION 4: Retrain on full dataset with best parameters\n",
        "print(\"\\nRetraining best model on full training set...\")\n",
        "# Extract best parameters and create final model\n",
        "final_nn_params = nn_best_params.copy()\n",
        "\n",
        "# Remove the 'classifier__' prefix\n",
        "final_nn_params = {k.replace('classifier__', ''): v for k, v in final_nn_params.items()}\n",
        "\n",
        "# Create final model with best parameters\n",
        "final_nn_model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', MLPClassifier(\n",
        "        **final_nn_params,\n",
        "        random_state=42,\n",
        "        max_iter=500,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1,\n",
        "        n_iter_no_change=15,\n",
        "        verbose=False\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Train on full dataset\n",
        "start_time = time.time()\n",
        "final_nn_model.fit(X_train, y_train)\n",
        "full_training_time = time.time() - start_time\n",
        "print(f\"✓ Full training completed in {full_training_time:.2f} seconds\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_nn = final_nn_model.predict(X_test)\n",
        "y_pred_proba_nn = final_nn_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"✓ Neural Network predictions ready for evaluation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5btHJ22gnwhD",
        "outputId": "d7b0a93b-be97-4fe5-cec1-0d2fd18e93d2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================================================================\n",
            "STEP 6: NEURAL NETWORK CLASSIFIER - OPTIMIZED\n",
            "======================================================================\n",
            "Class weights for Neural Network: {0: np.float64(0.6805991785455424), 1: np.float64(1.8842809364548494)}\n",
            "Using sampled data for GridSearch: 2000 samples (original: 5634)\n",
            "\n",
            "Optimized Hyperparameter Grid:\n",
            "  classifier__hidden_layer_sizes: [(50,), (100,)]\n",
            "  classifier__activation: ['relu']\n",
            "  classifier__alpha: [0.001, 0.01]\n",
            "  classifier__solver: ['adam']\n",
            "  classifier__batch_size: [64, 128]\n",
            "\n",
            "Performing OPTIMIZED Grid Search for Neural Network...\n",
            "This will be much faster than the original version...\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.001, classifier__batch_size=64, classifier__hidden_layer_sizes=(50,), classifier__solver=adam; total time=   0.2s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.001, classifier__batch_size=64, classifier__hidden_layer_sizes=(50,), classifier__solver=adam; total time=   0.3s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.001, classifier__batch_size=64, classifier__hidden_layer_sizes=(50,), classifier__solver=adam; total time=   0.3s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.001, classifier__batch_size=64, classifier__hidden_layer_sizes=(100,), classifier__solver=adam; total time=   0.3s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.001, classifier__batch_size=64, classifier__hidden_layer_sizes=(100,), classifier__solver=adam; total time=   0.3s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.001, classifier__batch_size=64, classifier__hidden_layer_sizes=(100,), classifier__solver=adam; total time=   0.3s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.001, classifier__batch_size=128, classifier__hidden_layer_sizes=(50,), classifier__solver=adam; total time=   0.2s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.001, classifier__batch_size=128, classifier__hidden_layer_sizes=(50,), classifier__solver=adam; total time=   0.2s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.001, classifier__batch_size=128, classifier__hidden_layer_sizes=(50,), classifier__solver=adam; total time=   0.2s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.001, classifier__batch_size=128, classifier__hidden_layer_sizes=(100,), classifier__solver=adam; total time=   0.3s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.001, classifier__batch_size=128, classifier__hidden_layer_sizes=(100,), classifier__solver=adam; total time=   0.3s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.001, classifier__batch_size=128, classifier__hidden_layer_sizes=(100,), classifier__solver=adam; total time=   0.3s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.01, classifier__batch_size=64, classifier__hidden_layer_sizes=(50,), classifier__solver=adam; total time=   0.2s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.01, classifier__batch_size=64, classifier__hidden_layer_sizes=(50,), classifier__solver=adam; total time=   0.3s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.01, classifier__batch_size=64, classifier__hidden_layer_sizes=(50,), classifier__solver=adam; total time=   0.3s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.01, classifier__batch_size=64, classifier__hidden_layer_sizes=(100,), classifier__solver=adam; total time=   0.3s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.01, classifier__batch_size=64, classifier__hidden_layer_sizes=(100,), classifier__solver=adam; total time=   0.3s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.01, classifier__batch_size=64, classifier__hidden_layer_sizes=(100,), classifier__solver=adam; total time=   0.3s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.01, classifier__batch_size=128, classifier__hidden_layer_sizes=(50,), classifier__solver=adam; total time=   0.2s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.01, classifier__batch_size=128, classifier__hidden_layer_sizes=(50,), classifier__solver=adam; total time=   0.2s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.01, classifier__batch_size=128, classifier__hidden_layer_sizes=(50,), classifier__solver=adam; total time=   0.2s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.01, classifier__batch_size=128, classifier__hidden_layer_sizes=(100,), classifier__solver=adam; total time=   0.3s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.01, classifier__batch_size=128, classifier__hidden_layer_sizes=(100,), classifier__solver=adam; total time=   0.3s\n",
            "[CV] END classifier__activation=relu, classifier__alpha=0.01, classifier__batch_size=128, classifier__hidden_layer_sizes=(100,), classifier__solver=adam; total time=   0.3s\n",
            "✓ Grid Search completed in 0.12 minutes (7.20 seconds)\n",
            "\n",
            "Best Neural Network Parameters:\n",
            "  classifier__activation: relu\n",
            "  classifier__alpha: 0.001\n",
            "  classifier__batch_size: 64\n",
            "  classifier__hidden_layer_sizes: (100,)\n",
            "  classifier__solver: adam\n",
            "\n",
            "Best CV Score (F1): 0.5693\n",
            "\n",
            "Retraining best model on full training set...\n",
            "✓ Full training completed in 3.24 seconds\n",
            "✓ Neural Network predictions ready for evaluation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"STEP 7: MODEL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def evaluate_model(y_true, y_pred, y_pred_proba, model_name):\n",
        "    \"\"\"Calculate and display evaluation metrics\"\"\"\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    # Additional metrics\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
        "\n",
        "    print(f\"\\n{model_name} Performance:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
        "    print(f\"Specificity: {specificity:.4f}\")\n",
        "    print(f\"NPV:        {npv:.4f}\")\n",
        "\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(f\"True Negatives:  {tn}\")\n",
        "    print(f\"False Positives: {fp}\")\n",
        "    print(f\"False Negatives: {fn}\")\n",
        "    print(f\"True Positives:  {tp}\")\n",
        "\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=['No Churn', 'Churn']))\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'specificity': specificity,\n",
        "        'npv': npv,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "# Evaluate Decision Tree\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "dt_metrics = evaluate_model(y_test, y_pred_dt, y_pred_proba_dt, \"DECISION TREE\")\n",
        "\n",
        "# Evaluate Neural Network\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "nn_metrics = evaluate_model(y_test, y_pred_nn, y_pred_proba_nn, \"NEURAL NETWORK\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLwl9RNhoZd7",
        "outputId": "dbee2cd7-f28b-41b0-cfde-0054a0f872cd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================================================================\n",
            "STEP 7: MODEL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "==================================================\n",
            "\n",
            "DECISION TREE Performance:\n",
            "--------------------------------------------------\n",
            "Accuracy:  0.7488\n",
            "Precision: 0.5180\n",
            "Recall:    0.7701\n",
            "F1-Score:  0.6194\n",
            "ROC-AUC:   0.8183\n",
            "Specificity: 0.7411\n",
            "NPV:        0.8992\n",
            "\n",
            "Confusion Matrix:\n",
            "True Negatives:  767\n",
            "False Positives: 268\n",
            "False Negatives: 86\n",
            "True Positives:  288\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    No Churn       0.90      0.74      0.81      1035\n",
            "       Churn       0.52      0.77      0.62       374\n",
            "\n",
            "    accuracy                           0.75      1409\n",
            "   macro avg       0.71      0.76      0.72      1409\n",
            "weighted avg       0.80      0.75      0.76      1409\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "NEURAL NETWORK Performance:\n",
            "--------------------------------------------------\n",
            "Accuracy:  0.8027\n",
            "Precision: 0.6690\n",
            "Recall:    0.5080\n",
            "F1-Score:  0.5775\n",
            "ROC-AUC:   0.8457\n",
            "Specificity: 0.9092\n",
            "NPV:        0.8364\n",
            "\n",
            "Confusion Matrix:\n",
            "True Negatives:  941\n",
            "False Positives: 94\n",
            "False Negatives: 184\n",
            "True Positives:  190\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    No Churn       0.84      0.91      0.87      1035\n",
            "       Churn       0.67      0.51      0.58       374\n",
            "\n",
            "    accuracy                           0.80      1409\n",
            "   macro avg       0.75      0.71      0.72      1409\n",
            "weighted avg       0.79      0.80      0.79      1409\n",
            "\n"
          ]
        }
      ]
    }
  ]
}